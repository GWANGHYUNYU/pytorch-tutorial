{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_2_mnist_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPESX7x8CDXQbqDLQ/OvnBy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GWANGHYUNYU/pytorch-tutorial/blob/master/09_2_mnist_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fivwIuGc3BTb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOrKmegjG5Ww"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEnN1gwFzaj5"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyI9Ke1VzcHp"
      },
      "source": [
        "# parameters\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLwKSVwwze15"
      },
      "source": [
        "# MNIST dataset\n",
        "mnist_train = torchvision.datasets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7RZzT0Mzexg"
      },
      "source": [
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dkKy0UklgMt"
      },
      "source": [
        "# MNIST data image of shape 28 * 28 = 784\n",
        "class MNIST_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(784, 256, bias=True).to(device)\n",
        "        self.linear2 = torch.nn.Linear(256, 256, bias=True).to(device)\n",
        "        self.linear3 = torch.nn.Linear(256, 10, bias=True).to(device)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        return x"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRKXtyevznRZ"
      },
      "source": [
        "# MNIST data image of shape 28 * 28 = 784\n",
        "class W_I_MNIST_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(784, 256, bias=True).to(device)\n",
        "        self.linear2 = torch.nn.Linear(256, 256, bias=True).to(device)\n",
        "        self.linear3 = torch.nn.Linear(256, 10, bias=True).to(device)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "        # Weight_Initialization\n",
        "        torch.nn.init.normal_(self.linear1.weight)\n",
        "        torch.nn.init.normal_(self.linear2.weight)\n",
        "        torch.nn.init.normal_(self.linear3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        return x"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKq5RiePz_Lh"
      },
      "source": [
        "# Initialization\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear1:\n",
        "        torch.nn.init.normal_(linear1.weight)\n",
        "        torch.nn.init.normal_(linear2.weight)\n",
        "        torch.nn.init.normal_(linear3.weight)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H5VotrXSz87"
      },
      "source": [
        "net = MNIST_Classifier()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNOpfbS4S414",
        "outputId": "ea574421-3290-48a0-b501-07222b42c17d"
      },
      "source": [
        "count = 1\n",
        "for para in net.parameters():\n",
        "    print(count, \"'th layer \", para.size())\n",
        "    print(para)\n",
        "    count += 1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 'th layer  torch.Size([256, 784])\n",
            "Parameter containing:\n",
            "tensor([[-2.9863e-02, -6.3304e-04, -6.9076e-03,  ..., -1.8779e-02,\n",
            "          3.0876e-02, -3.1737e-02],\n",
            "        [ 6.7273e-03,  2.0005e-02, -2.4505e-02,  ..., -2.7296e-02,\n",
            "         -3.8136e-03,  1.1463e-03],\n",
            "        [-1.7847e-02, -2.9822e-02,  7.4012e-05,  ..., -3.2111e-02,\n",
            "          2.6378e-02,  3.2280e-02],\n",
            "        ...,\n",
            "        [-6.6998e-03, -3.3317e-02, -6.8987e-03,  ...,  3.3257e-02,\n",
            "          3.1245e-02,  3.7429e-03],\n",
            "        [ 2.8821e-02,  2.5984e-02,  3.5331e-02,  ..., -2.5765e-02,\n",
            "         -2.5062e-02,  6.3487e-03],\n",
            "        [ 2.2188e-02, -1.8242e-02,  5.2263e-03,  ...,  3.0406e-02,\n",
            "         -2.2001e-02, -1.7911e-02]], requires_grad=True)\n",
            "2 'th layer  torch.Size([256])\n",
            "Parameter containing:\n",
            "tensor([ 0.0018, -0.0132, -0.0227, -0.0255,  0.0308,  0.0288, -0.0201,  0.0258,\n",
            "         0.0337,  0.0178, -0.0159, -0.0091,  0.0232,  0.0081,  0.0028,  0.0300,\n",
            "        -0.0147, -0.0262, -0.0349,  0.0065, -0.0175, -0.0297,  0.0337, -0.0258,\n",
            "         0.0141, -0.0208,  0.0275,  0.0266,  0.0016,  0.0294,  0.0209, -0.0075,\n",
            "        -0.0173, -0.0171,  0.0063,  0.0166, -0.0014,  0.0221,  0.0355,  0.0282,\n",
            "        -0.0092, -0.0313,  0.0177,  0.0142, -0.0139,  0.0280, -0.0033,  0.0329,\n",
            "         0.0121, -0.0144, -0.0198,  0.0134,  0.0244,  0.0272, -0.0040, -0.0182,\n",
            "         0.0278, -0.0041,  0.0022, -0.0151,  0.0287,  0.0302,  0.0159,  0.0302,\n",
            "         0.0084,  0.0316,  0.0081, -0.0330,  0.0241,  0.0048, -0.0173,  0.0119,\n",
            "         0.0111,  0.0123, -0.0241,  0.0087,  0.0102, -0.0110,  0.0326,  0.0203,\n",
            "        -0.0282, -0.0217,  0.0154,  0.0302, -0.0116,  0.0097,  0.0202,  0.0139,\n",
            "        -0.0140,  0.0342,  0.0108, -0.0063, -0.0299,  0.0092,  0.0271,  0.0024,\n",
            "         0.0196,  0.0233,  0.0261, -0.0107,  0.0095,  0.0133, -0.0160, -0.0186,\n",
            "         0.0140, -0.0099,  0.0091,  0.0261,  0.0317,  0.0225, -0.0277, -0.0024,\n",
            "         0.0190, -0.0179, -0.0025, -0.0247,  0.0101,  0.0311,  0.0082, -0.0238,\n",
            "         0.0041,  0.0168,  0.0096, -0.0092,  0.0295,  0.0050,  0.0245,  0.0082,\n",
            "        -0.0347, -0.0356, -0.0111, -0.0125, -0.0105, -0.0294,  0.0068,  0.0230,\n",
            "        -0.0316,  0.0018,  0.0004,  0.0162, -0.0316,  0.0321,  0.0207,  0.0143,\n",
            "        -0.0356, -0.0059, -0.0126, -0.0278,  0.0316,  0.0316, -0.0325,  0.0201,\n",
            "        -0.0263, -0.0005, -0.0032, -0.0288, -0.0058, -0.0017,  0.0064,  0.0134,\n",
            "         0.0343,  0.0294,  0.0091,  0.0097, -0.0080, -0.0212,  0.0337, -0.0334,\n",
            "        -0.0267,  0.0065, -0.0021, -0.0042, -0.0173,  0.0294,  0.0350,  0.0318,\n",
            "        -0.0214, -0.0089,  0.0119, -0.0088,  0.0266,  0.0296,  0.0305, -0.0019,\n",
            "         0.0225,  0.0016,  0.0283, -0.0283, -0.0065, -0.0220, -0.0113,  0.0261,\n",
            "        -0.0264, -0.0090,  0.0043,  0.0206, -0.0027, -0.0348, -0.0057, -0.0184,\n",
            "        -0.0109,  0.0293, -0.0130, -0.0059, -0.0329,  0.0247, -0.0211,  0.0288,\n",
            "         0.0346,  0.0182, -0.0328,  0.0104,  0.0169, -0.0149, -0.0328,  0.0059,\n",
            "        -0.0100, -0.0158, -0.0017, -0.0267,  0.0153, -0.0086,  0.0331,  0.0195,\n",
            "         0.0081, -0.0311,  0.0339,  0.0329,  0.0135, -0.0218, -0.0074, -0.0100,\n",
            "         0.0272, -0.0320, -0.0198, -0.0334, -0.0015,  0.0249, -0.0204, -0.0346,\n",
            "         0.0017, -0.0115,  0.0275, -0.0185, -0.0338, -0.0215,  0.0323,  0.0145,\n",
            "         0.0325, -0.0065,  0.0237, -0.0256, -0.0113, -0.0075, -0.0127, -0.0223],\n",
            "       requires_grad=True)\n",
            "3 'th layer  torch.Size([256, 256])\n",
            "Parameter containing:\n",
            "tensor([[ 0.0451, -0.0357,  0.0190,  ..., -0.0552,  0.0336, -0.0617],\n",
            "        [-0.0510, -0.0129,  0.0478,  ..., -0.0378, -0.0377,  0.0506],\n",
            "        [-0.0569,  0.0526,  0.0490,  ..., -0.0145, -0.0281,  0.0443],\n",
            "        ...,\n",
            "        [-0.0574,  0.0220,  0.0512,  ...,  0.0398, -0.0559, -0.0428],\n",
            "        [-0.0433, -0.0407,  0.0508,  ...,  0.0113, -0.0109, -0.0168],\n",
            "        [ 0.0458, -0.0025,  0.0405,  ..., -0.0545, -0.0185, -0.0377]],\n",
            "       requires_grad=True)\n",
            "4 'th layer  torch.Size([256])\n",
            "Parameter containing:\n",
            "tensor([ 0.0259, -0.0316,  0.0343, -0.0096, -0.0570, -0.0526, -0.0614, -0.0187,\n",
            "        -0.0554, -0.0114, -0.0109, -0.0357, -0.0393, -0.0218, -0.0001,  0.0566,\n",
            "        -0.0516,  0.0507,  0.0496,  0.0221,  0.0591,  0.0208, -0.0122,  0.0007,\n",
            "        -0.0516,  0.0267,  0.0604,  0.0540, -0.0394,  0.0373,  0.0056,  0.0009,\n",
            "         0.0073, -0.0209,  0.0367,  0.0390, -0.0266,  0.0487,  0.0145, -0.0047,\n",
            "         0.0228, -0.0121, -0.0438,  0.0269, -0.0303,  0.0351, -0.0591, -0.0534,\n",
            "         0.0241, -0.0537, -0.0495,  0.0376,  0.0306,  0.0595,  0.0263,  0.0062,\n",
            "        -0.0439, -0.0534,  0.0369, -0.0506,  0.0488,  0.0613, -0.0506, -0.0489,\n",
            "        -0.0605, -0.0331, -0.0127,  0.0380, -0.0134,  0.0184, -0.0618, -0.0282,\n",
            "         0.0016,  0.0388, -0.0262, -0.0208,  0.0466,  0.0434,  0.0437,  0.0482,\n",
            "         0.0377,  0.0304,  0.0176, -0.0039,  0.0012,  0.0194, -0.0054, -0.0543,\n",
            "         0.0428, -0.0416,  0.0312,  0.0327,  0.0242, -0.0169,  0.0142, -0.0325,\n",
            "         0.0325, -0.0375,  0.0589,  0.0603,  0.0397,  0.0267, -0.0513, -0.0479,\n",
            "         0.0431, -0.0324,  0.0569, -0.0315, -0.0176, -0.0301,  0.0253, -0.0145,\n",
            "         0.0477, -0.0016, -0.0603,  0.0415, -0.0475, -0.0508, -0.0213, -0.0444,\n",
            "        -0.0381, -0.0451,  0.0219, -0.0418, -0.0479, -0.0293, -0.0461, -0.0027,\n",
            "        -0.0390, -0.0383,  0.0324,  0.0302,  0.0160, -0.0315, -0.0528,  0.0393,\n",
            "        -0.0423,  0.0042, -0.0301, -0.0330,  0.0385,  0.0007,  0.0067, -0.0269,\n",
            "         0.0359,  0.0033,  0.0320,  0.0251,  0.0479,  0.0140,  0.0290,  0.0592,\n",
            "         0.0115, -0.0240, -0.0550,  0.0559,  0.0326,  0.0453,  0.0322,  0.0372,\n",
            "         0.0293,  0.0299,  0.0192,  0.0312, -0.0096, -0.0502,  0.0164, -0.0127,\n",
            "        -0.0124, -0.0193,  0.0452, -0.0337,  0.0424, -0.0388,  0.0431,  0.0623,\n",
            "        -0.0533, -0.0619,  0.0402,  0.0088,  0.0458, -0.0443,  0.0315,  0.0013,\n",
            "        -0.0030,  0.0345, -0.0177,  0.0189, -0.0449, -0.0051,  0.0158, -0.0312,\n",
            "         0.0473, -0.0233,  0.0208, -0.0156,  0.0220,  0.0303,  0.0535, -0.0492,\n",
            "        -0.0487, -0.0319, -0.0601,  0.0086,  0.0109, -0.0034, -0.0138, -0.0208,\n",
            "        -0.0195,  0.0313,  0.0530, -0.0220, -0.0137,  0.0157,  0.0562,  0.0070,\n",
            "         0.0363,  0.0500,  0.0431,  0.0238,  0.0236, -0.0573,  0.0495, -0.0562,\n",
            "         0.0465,  0.0547,  0.0275,  0.0202,  0.0166, -0.0577,  0.0087, -0.0102,\n",
            "        -0.0370, -0.0289, -0.0056,  0.0391, -0.0233,  0.0585,  0.0318,  0.0426,\n",
            "        -0.0491,  0.0229, -0.0151,  0.0010,  0.0351, -0.0615,  0.0544,  0.0371,\n",
            "        -0.0599,  0.0493, -0.0422, -0.0201, -0.0242,  0.0169,  0.0413, -0.0119],\n",
            "       requires_grad=True)\n",
            "5 'th layer  torch.Size([10, 256])\n",
            "Parameter containing:\n",
            "tensor([[-0.0425,  0.0077, -0.0456,  ...,  0.0343,  0.0021,  0.0172],\n",
            "        [ 0.0543, -0.0163,  0.0044,  ...,  0.0057,  0.0321, -0.0576],\n",
            "        [-0.0529,  0.0624, -0.0435,  ...,  0.0035,  0.0463,  0.0107],\n",
            "        ...,\n",
            "        [-0.0080,  0.0433,  0.0119,  ...,  0.0042, -0.0053, -0.0376],\n",
            "        [ 0.0382, -0.0334,  0.0115,  ...,  0.0518,  0.0258, -0.0208],\n",
            "        [-0.0131,  0.0198, -0.0557,  ...,  0.0581, -0.0100, -0.0607]],\n",
            "       requires_grad=True)\n",
            "6 'th layer  torch.Size([10])\n",
            "Parameter containing:\n",
            "tensor([-0.0189,  0.0355, -0.0301, -0.0442,  0.0095,  0.0578, -0.0164,  0.0215,\n",
            "        -0.0482, -0.0625], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJWQTGBU188h"
      },
      "source": [
        "net2 = W_I_MNIST_Classifier()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi4096PJlt5V",
        "outputId": "a1307069-c242-4a61-eff2-a08e5ba618b1"
      },
      "source": [
        "count = 1\n",
        "for para in net2.parameters():\n",
        "    print(count, \"'th layer \", para.size())\n",
        "    print(para)\n",
        "    count += 1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 'th layer  torch.Size([256, 784])\n",
            "Parameter containing:\n",
            "tensor([[-1.1596,  0.4280,  0.7997,  ..., -1.4786,  0.9252,  0.5768],\n",
            "        [ 0.4508,  0.6853, -0.2222,  ...,  0.8277, -0.9715,  0.5756],\n",
            "        [-1.0778, -0.5807, -1.7917,  ..., -0.9397, -0.3643, -0.2321],\n",
            "        ...,\n",
            "        [ 0.6008, -1.3543,  0.1078,  ...,  0.4966, -0.5438,  0.8466],\n",
            "        [ 0.1973, -0.0807, -2.5902,  ..., -0.9303, -0.0072, -0.2866],\n",
            "        [-0.4717,  2.8090, -0.1680,  ..., -0.4490, -0.3798,  0.8595]],\n",
            "       requires_grad=True)\n",
            "2 'th layer  torch.Size([256])\n",
            "Parameter containing:\n",
            "tensor([-3.4435e-02,  8.2310e-03, -3.4367e-02,  6.2159e-05, -2.3674e-02,\n",
            "         3.1226e-02, -1.4099e-02,  2.9567e-02, -3.5241e-02, -1.5082e-02,\n",
            "        -3.0629e-02, -2.7913e-02, -3.4809e-03,  8.5663e-03,  1.8764e-03,\n",
            "        -5.4471e-03,  1.5817e-02, -2.7542e-02, -2.9589e-02,  2.0178e-03,\n",
            "         1.2220e-02, -2.6729e-02,  3.9311e-03,  3.1528e-02, -3.1323e-02,\n",
            "         6.3061e-03, -2.2343e-03, -1.1955e-02,  2.3780e-02, -2.5828e-02,\n",
            "         1.9037e-02,  2.3432e-02, -8.4838e-03, -4.2103e-03, -3.1240e-02,\n",
            "         1.6967e-02,  3.7582e-03,  3.2057e-02,  3.1673e-02, -4.7397e-03,\n",
            "        -2.3355e-02,  2.6325e-02,  2.0325e-02,  1.7728e-02, -2.6349e-02,\n",
            "        -5.0047e-03,  1.2583e-02,  2.9431e-02, -1.7097e-02, -1.3841e-03,\n",
            "        -1.8896e-02, -9.6451e-03,  2.4755e-02,  1.4730e-02,  1.9491e-02,\n",
            "         1.4385e-02,  2.7019e-02, -2.5801e-02,  5.9138e-03, -1.6576e-02,\n",
            "         2.7546e-02,  1.5412e-02, -3.5671e-02, -2.8052e-02,  3.2619e-02,\n",
            "         1.0337e-02, -2.6204e-02, -2.7203e-02, -2.5260e-02,  1.3494e-02,\n",
            "         4.3506e-04, -1.5269e-02,  5.2619e-03,  3.0888e-02, -2.3922e-02,\n",
            "         1.4578e-02, -1.4340e-02,  1.1072e-02, -4.7577e-03,  1.9532e-02,\n",
            "         3.0377e-02, -8.9786e-03,  2.5067e-02, -3.0862e-02,  8.9430e-03,\n",
            "        -1.7150e-02, -1.4510e-02, -3.5609e-02, -1.6917e-02,  2.5419e-02,\n",
            "        -1.5371e-02,  1.6407e-02, -2.8610e-02,  4.0574e-03,  3.5350e-02,\n",
            "         3.1318e-02, -1.1982e-02, -2.2639e-02,  1.2874e-02,  3.1223e-02,\n",
            "        -7.1245e-03, -1.2570e-02, -3.1222e-02,  3.4668e-02, -6.5269e-03,\n",
            "         3.2702e-02, -1.0394e-02,  1.0765e-03,  3.5025e-02, -3.5169e-02,\n",
            "         7.8956e-03, -2.8374e-02,  1.7696e-02,  3.3425e-02, -2.8730e-02,\n",
            "         3.4907e-02,  2.5790e-02,  3.0618e-02,  2.9640e-02,  3.5229e-02,\n",
            "         1.0112e-02,  1.3447e-03, -3.4686e-02,  9.2788e-03, -9.9441e-03,\n",
            "        -1.0905e-02,  2.7013e-02,  2.5897e-02,  5.2815e-04, -1.3168e-02,\n",
            "        -1.6475e-02, -9.0192e-03,  3.3048e-02,  3.3143e-02,  2.5302e-02,\n",
            "         3.9120e-03,  3.2511e-02, -2.6536e-02, -6.3167e-03,  3.3574e-02,\n",
            "        -2.8010e-02, -1.2932e-02, -1.1033e-02,  1.6920e-02, -2.6315e-02,\n",
            "         2.2447e-02,  1.4595e-02,  3.0023e-02,  1.6088e-02, -1.9660e-02,\n",
            "         1.6119e-02,  2.6764e-02, -3.3956e-02,  3.7614e-03, -1.7209e-02,\n",
            "         1.9953e-02, -6.1421e-03,  3.4472e-02,  3.4273e-02,  4.2102e-03,\n",
            "        -3.0970e-02,  2.9971e-02,  2.9401e-03, -5.2864e-03, -1.2086e-02,\n",
            "         7.8271e-03,  2.6735e-02, -2.3988e-02, -2.4401e-02,  1.2718e-02,\n",
            "         9.6050e-03,  3.4835e-02, -2.6104e-02, -2.8254e-02,  1.3767e-02,\n",
            "         3.4417e-02,  6.1515e-04,  5.7974e-03,  1.9990e-02, -8.3997e-03,\n",
            "         7.8352e-03, -2.9982e-02, -2.8234e-02, -9.0648e-03, -1.9258e-02,\n",
            "         4.0196e-03, -3.0831e-02, -2.8761e-02,  1.6025e-02, -6.5689e-03,\n",
            "         1.7158e-02,  2.2935e-02, -2.6006e-02, -6.4377e-03,  2.1272e-04,\n",
            "         2.6114e-02,  1.0720e-04, -4.3572e-03, -1.9565e-03,  1.6936e-02,\n",
            "         2.6058e-02, -4.7170e-03, -4.2489e-03,  5.2272e-03,  2.6044e-02,\n",
            "        -4.5593e-03,  2.2311e-03,  6.1589e-03, -1.8407e-02,  2.9412e-02,\n",
            "         3.3739e-03, -1.0827e-02, -1.0855e-02,  2.7207e-02,  3.3508e-03,\n",
            "        -1.3136e-03,  4.8812e-03,  2.6581e-02,  5.7628e-03,  1.3755e-02,\n",
            "         2.9448e-02, -3.3161e-02,  2.7715e-02,  2.8139e-02, -8.0994e-03,\n",
            "         2.0618e-02, -3.4844e-02,  3.5070e-02,  1.5499e-02, -2.9352e-02,\n",
            "        -1.1099e-02,  3.5119e-02,  6.4121e-04, -1.7990e-02,  1.2632e-02,\n",
            "         1.1746e-02, -2.3696e-02,  4.3571e-03,  2.1193e-02, -5.8027e-03,\n",
            "        -9.5992e-03,  3.3668e-02, -3.1702e-02, -1.1664e-02, -2.2487e-02,\n",
            "         1.1583e-02, -3.0257e-02, -3.0321e-02,  9.5122e-03,  1.7173e-02,\n",
            "         2.1571e-02, -2.9797e-02, -2.7976e-02,  9.0695e-03, -1.7862e-02,\n",
            "        -9.1938e-04], requires_grad=True)\n",
            "3 'th layer  torch.Size([256, 256])\n",
            "Parameter containing:\n",
            "tensor([[-0.9565, -0.4439, -0.6748,  ...,  0.3157, -0.1853, -0.9513],\n",
            "        [ 0.3681,  1.1277,  1.0291,  ..., -0.4020, -0.6540,  0.9004],\n",
            "        [ 0.0779, -1.4595,  0.0213,  ...,  0.6525, -0.6331, -0.1145],\n",
            "        ...,\n",
            "        [-0.1177, -0.7693, -0.0583,  ..., -2.4343,  1.0781, -0.0764],\n",
            "        [ 2.2292, -0.5980, -0.9428,  ..., -0.6123,  0.6423, -0.5656],\n",
            "        [-0.0474, -0.6411,  0.3984,  ..., -1.3798, -1.0043,  1.1318]],\n",
            "       requires_grad=True)\n",
            "4 'th layer  torch.Size([256])\n",
            "Parameter containing:\n",
            "tensor([ 0.0032,  0.0573,  0.0383,  0.0349, -0.0188, -0.0481,  0.0113, -0.0413,\n",
            "        -0.0033, -0.0578, -0.0014, -0.0509,  0.0093,  0.0443,  0.0280, -0.0558,\n",
            "         0.0202,  0.0255,  0.0117, -0.0213,  0.0417,  0.0276,  0.0444,  0.0273,\n",
            "        -0.0343,  0.0097, -0.0422,  0.0037,  0.0528,  0.0390,  0.0324, -0.0483,\n",
            "        -0.0090,  0.0317, -0.0059, -0.0372,  0.0167,  0.0295,  0.0482, -0.0060,\n",
            "        -0.0455, -0.0463, -0.0604, -0.0249,  0.0080, -0.0037, -0.0600,  0.0215,\n",
            "         0.0256, -0.0085,  0.0493,  0.0115,  0.0366, -0.0182, -0.0207,  0.0525,\n",
            "        -0.0389,  0.0439,  0.0454, -0.0080, -0.0534, -0.0396,  0.0034, -0.0172,\n",
            "        -0.0440,  0.0604, -0.0484, -0.0497, -0.0592, -0.0403, -0.0285, -0.0220,\n",
            "        -0.0560, -0.0166,  0.0087, -0.0217, -0.0567,  0.0420, -0.0044,  0.0486,\n",
            "         0.0595, -0.0568, -0.0202,  0.0410,  0.0558,  0.0336, -0.0171,  0.0213,\n",
            "         0.0390, -0.0194, -0.0557, -0.0055,  0.0059,  0.0074, -0.0227,  0.0383,\n",
            "        -0.0449,  0.0010,  0.0429,  0.0554, -0.0574,  0.0472,  0.0104, -0.0207,\n",
            "         0.0585,  0.0573, -0.0133, -0.0563,  0.0177, -0.0129, -0.0468,  0.0091,\n",
            "        -0.0048, -0.0250, -0.0286, -0.0482,  0.0086, -0.0256,  0.0375,  0.0270,\n",
            "         0.0152, -0.0381,  0.0248, -0.0241, -0.0407, -0.0537,  0.0224,  0.0103,\n",
            "         0.0486,  0.0189,  0.0495,  0.0059, -0.0177,  0.0360, -0.0544,  0.0588,\n",
            "         0.0100, -0.0621, -0.0072,  0.0509,  0.0126,  0.0126, -0.0386,  0.0107,\n",
            "         0.0312, -0.0510, -0.0520, -0.0625, -0.0474,  0.0217,  0.0524, -0.0399,\n",
            "        -0.0569, -0.0525,  0.0579, -0.0454,  0.0214, -0.0581,  0.0438,  0.0471,\n",
            "         0.0195,  0.0227,  0.0015, -0.0190, -0.0297,  0.0065,  0.0129, -0.0143,\n",
            "        -0.0294,  0.0465, -0.0146, -0.0461, -0.0373, -0.0223, -0.0176,  0.0039,\n",
            "         0.0512,  0.0325, -0.0479, -0.0384,  0.0267,  0.0591,  0.0615,  0.0049,\n",
            "         0.0105, -0.0470, -0.0018, -0.0279,  0.0562, -0.0234, -0.0238, -0.0012,\n",
            "        -0.0281,  0.0083,  0.0139,  0.0542,  0.0547, -0.0551,  0.0573, -0.0585,\n",
            "         0.0206, -0.0549,  0.0206, -0.0581, -0.0586, -0.0392,  0.0287, -0.0340,\n",
            "         0.0556,  0.0191, -0.0035, -0.0411,  0.0469,  0.0391,  0.0558,  0.0384,\n",
            "        -0.0604,  0.0276,  0.0102,  0.0254, -0.0216,  0.0007, -0.0408,  0.0505,\n",
            "         0.0190, -0.0034, -0.0445, -0.0233,  0.0229,  0.0156,  0.0156,  0.0343,\n",
            "        -0.0535,  0.0225,  0.0326, -0.0458,  0.0381, -0.0612, -0.0347, -0.0451,\n",
            "        -0.0168, -0.0443, -0.0463,  0.0063, -0.0584,  0.0428, -0.0484, -0.0384,\n",
            "        -0.0112,  0.0135,  0.0555,  0.0065, -0.0188,  0.0406,  0.0607, -0.0126],\n",
            "       requires_grad=True)\n",
            "5 'th layer  torch.Size([10, 256])\n",
            "Parameter containing:\n",
            "tensor([[-0.9550,  1.5267, -0.0808,  ..., -0.0317, -0.4215, -0.6433],\n",
            "        [-1.0400,  1.6163, -0.5384,  ...,  0.9660,  1.1220, -0.7217],\n",
            "        [ 2.4426,  1.6043,  1.1029,  ..., -0.3574,  0.7383,  1.6834],\n",
            "        ...,\n",
            "        [-0.5243, -1.5380, -1.1929,  ..., -0.9999,  2.0011,  0.0241],\n",
            "        [ 0.4396,  0.7371,  0.1522,  ..., -1.0744,  3.1176, -0.2660],\n",
            "        [-1.9028,  0.2665,  0.5692,  ...,  0.7941,  0.7549,  0.0538]],\n",
            "       requires_grad=True)\n",
            "6 'th layer  torch.Size([10])\n",
            "Parameter containing:\n",
            "tensor([-0.0583, -0.0104, -0.0603,  0.0379, -0.0396, -0.0333,  0.0055,  0.0078,\n",
            "        -0.0542, -0.0444], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro1j5Xmu2Sgw"
      },
      "source": [
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ4LhPah2gVw",
        "outputId": "00d80d0a-dc3a-4522-cdb8-32ab4303ecc3"
      },
      "source": [
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = net(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 0.295117170\n",
            "Epoch: 0002 cost = 0.110606916\n",
            "Epoch: 0003 cost = 0.072952934\n",
            "Epoch: 0004 cost = 0.052162699\n",
            "Epoch: 0005 cost = 0.039112888\n",
            "Epoch: 0006 cost = 0.031360138\n",
            "Epoch: 0007 cost = 0.025191443\n",
            "Epoch: 0008 cost = 0.021900330\n",
            "Epoch: 0009 cost = 0.017230988\n",
            "Epoch: 0010 cost = 0.015546730\n",
            "Epoch: 0011 cost = 0.015122313\n",
            "Epoch: 0012 cost = 0.014507786\n",
            "Epoch: 0013 cost = 0.010125137\n",
            "Epoch: 0014 cost = 0.011617439\n",
            "Epoch: 0015 cost = 0.008421371\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Bqo02K9j2pr5",
        "outputId": "e4313fe8-87cd-4ee3-a386-750db834884e"
      },
      "source": [
        "# Test the model using test sets\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = net(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = net(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
        "\n",
        "    \n",
        "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
        "    plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:69: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:59: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9783999919891357\n",
            "Label:  8\n",
            "Prediction:  8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}